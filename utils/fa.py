import pandas as pd
import os, sys, importlib
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


## fa
from factor_analyzer.factor_analyzer import calculate_kmo
from factor_analyzer import FactorAnalyzer
from pingouin import cronbach_alpha
from sklearn.decomposition import PCA
from mpl_toolkits.mplot3d import Axes3D



## my utils
from utils.io import save_csv_as_latex

def run_check_fa(DF, ITEMS, OUTPUT_FOLDER, 
                N_FACTORS=5, ROTATION="oblimin", 
                FA_NAMES=None,
                FA_ITEMS_DICT=None,
                get_items_heatmap=True,
                get_items_table=True, low_comm_t=0.5,
                get_factor_scores_by='both',
                run_scores_fa_on="both",
                get_cronbachs_a=True,
                get_barplot=False, get_pca3d=False, 
                ):
    
    
    # 英语提示，但是图片标题用法语！
    #output
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    print(f"[INFO] all results saved to folder '{OUTPUT_FOLDER}'!\n"
          f"[CHECK] INITIALISE DF, or MERGE fa scores will go WRONG!!!")

    print(f"============================RUN FA=================================")
    #-------------------------- check data -----------------------------
    missing_labels = [col for col in ITEMS if col not in DF.columns]
    if missing_labels:
        print(f"[WARNING] These items are missing in DF: {missing_labels}\n")

    else :
        df_dropna=DF[DF[ITEMS].sum(axis=1)!=0]
        data=df_dropna[ITEMS]
        print(f"[INFO] len scores in df notna:{len(data)}\n")
    
    # ---------------------------kmo-----------------------------------
    kmo_all, kmo_model = calculate_kmo(data)
    print(f"[INFO] kmo_model score (> 0.7) :{kmo_model}\n")


    # ---------------------------fa fit -------------------------------
    # if ROTATION =="oblimin":
    #     print(f"[INFO] oblimin allows cross-loadings!\n")
        
    fa = FactorAnalyzer(n_factors=N_FACTORS, rotation=ROTATION, method='minres')
    fa.fit(data)
    ### output impo :fa, data


    print("---------------------------heatmap of items-------------------------------")
    if get_items_heatmap==True: 
        loadings = fa.loadings_
        print(f"[INFO] good loadings > 0.5")
        
        if FA_NAMES :
            print(f"Rename loadings df with FA_NAMES!")
            loadings_df=pd.DataFrame(loadings, index=data.columns, columns=FA_NAMES)
        else :
            loadings_df=pd.DataFrame(loadings, index=data.columns)  
    
        plt.figure(figsize=(10,8))
        sns.heatmap(loadings_df, annot=True, cmap='coolwarm')
        plt.title('Charges factorielles des items')
        # plt.xticks(rotation=45)
        plt.tight_layout()
        outpath_heatmap=os.path.join(OUTPUT_FOLDER, 'heatmap_fa.jpeg')
        plt.savefig(outpath_heatmap, dpi=300)
        plt.show()
        print(f"[SAVE] heatmap of items saved to {outpath_heatmap}!\n")
    
    
    
    print(f'organise input FA_ITEMS_DICT as latex table'.center(100, '-'))
    if FA_ITEMS_DICT:# dict to csv
        fa_items_str_dict = {
            k: ", ".join(v)
            for k, v in FA_ITEMS_DICT.items()
        }

        fa_items_df=pd.DataFrame.from_dict(fa_items_str_dict, orient='index',columns=['items'])
        fa_items_df.reset_index(names='Tactique', inplace=True)
        display(fa_items_df)
               
        outpath_fa_items=os.path.join(OUTPUT_FOLDER, "latex","fa_items_table.tex")
        save_csv_as_latex(table_csv=fa_items_df, 
                          output_path=outpath_fa_items, 
                          caption="Bilan des items des tactiques de la présentation de soi", 
                          label="tab:fa_items_table", 
                          round=3,escape=True, index=True)
        
        

    if get_items_table==True:#***
        print("# ------------------------------------tab items fa------------------------------------")
        loadings = fa.loadings_
        communalities = fa.get_communalities()
        uniqueness = fa.get_uniquenesses()

        # gamma 和 sigma
        abs_loadings = np.abs(loadings)
        gamma = abs_loadings.max(axis=1)                   # 最大因子载荷 !
        sigma = np.sort(abs_loadings, axis=1)[:, -2]       # 第二大因子载荷!

        # get DataFrame
        desired_order = None  # 先定义，保证一定存在
        if FA_NAMES:
            fa_names=FA_NAMES
            ##如有fa输入按照这个整理!
            desired_order=['gamma','sigma','ouverture','authenticité','sociabilité',"auto_promotion","exemplarité",'communalité','spécificité']
            
        else :
            #没有FA_NAMES，则无法按照上面的顺序整理！
            fa_names=[f"Factor{i+1}" for i in range(0, loadings.shape[1])]
            
        columns = ['gamma', 'sigma'] + \
                fa_names + \
                ["communalité","spécificité"]
                # ['communality', 'uniqueness']

        df_fa = pd.DataFrame(
            np.column_stack([gamma, sigma, loadings, communalities, uniqueness]),
            index=data.columns,
            columns=columns
        )
        df_fa.round(3)
        
        # reorder 
        # desired_order=['gamma','sigma','ouverture','authenticité','sociabilité',"auto_promotion","exemplarité",'communalité','spécificité']
        if desired_order:
            df_fa=df_fa[desired_order]
        
        if FA_ITEMS_DICT and FA_NAMES:
            df_fa_num = df_fa.copy()# round的是时候不能有'-'
            factor_cols = FA_NAMES #选取因子列
            # 先把所有因子列设为 "-"
            df_fa.loc[:, factor_cols] = '-'

            # 再逐个因子填回“所属 item”的 loading
            for factor, items in FA_ITEMS_DICT.items():
                for item in items:
                    if item in df_fa.index and factor in df_fa.columns:
                        df_fa.loc[item, factor] = round(
                            float(df_fa_num.loc[item, factor]), 3
                        )
                        df_fa.loc[item]
        # df_fa=df_fa.round(4)

        display(df_fa)
        
    
            
        #info
        print(f"[INFO] mean COMM (> 0.6):{communalities.mean()}\n"
                    f"[INFO] mean UNIQ : {uniqueness.mean()}\n")
        low_comm_t=0.5
        low_comm_items = df_fa[df_fa['communalité'] <low_comm_t ]
        print(f"[WARNING] {len(low_comm_items)}/{len(df_fa)} low communality items (< {low_comm_t}):\n{';'.join(low_comm_items.index)}\n")



        # save as csv
        outpath_items_fa_csv=os.path.join(OUTPUT_FOLDER, 'items_fa.csv')
        df_fa.to_csv(outpath_items_fa_csv, index=True)##index!!!
        print(f"[SAVE] table saved to {outpath_items_fa_csv}!\n")
        
        # save as latex               
        outpath_items_fa_latex=os.path.join(OUTPUT_FOLDER, 'latex','items_fa.tex')        
        save_csv_as_latex(table_csv=df_fa, output_path=outpath_items_fa_latex, 
                          caption="Tableau de l'analyse factorielle des items", 
                          label="tab:items_fa", 
                          round=3,
                          escape=True,
                          index=True)
        
        
        
                
        ## ps.
        print(f"[WARNING] remember to RENAME factors by heatmap of zsc data!!!")            
    

    print("-------------------------------get fa scores(tactics)---------------------------------")
    # 从原始 DF 中筛选非空行 (df_dropna) 做 FA
    # 得到 FA scores (df_scores) (无id索引)！
    # 拼接回 df_dropna → 得到 df_tactics
    # 按 "host_about" 去重 → df_bio_tactics
    # 最后再 merge 回原始 DF → listings_tactics
    
    # init，空的df不会拼接上
    df_scores_mean=pd.DataFrame()
    df_scores_weighted=pd.DataFrame()
    desired_order_tactics=['ouverture','authenticité','sociabilité',"auto_promotion","exemplarité"]   
    # reorder for heatmap!
    
    if get_factor_scores_by  in ["mean","both"]:
        print(f"[INFO] calulate factor score by taking the average of its corresponding items, ref to a pre-defined map.\n")
        
        tac_item_map={
            "ouverture":['open to different cultures','cosmopolitan','international view','cultural exchange'],
            "authenticité":['personal life','life experiences','divers interests','hobbies','enjoy life'],
            'sociabilité':['meet new people', 'welcoming', 'friendly','sociable', 'interpersonal interaction'],
            'auto_promotion':['thoughtful service', 'attentive to needs','willing to help','responsive'],
            'exemplarité':["fan of Airbnb","Airbnb community",'love Airbnb', 'travel with Airbnb']
        }
        fa_scores_mean = pd.DataFrame(index=data.index)
        for f, items in tac_item_map.items():
            fa_scores_mean[f'{f}_mean'] = data[items].mean(axis=1)
        df_scores_mean=pd.DataFrame(fa_scores_mean).reset_index(drop=True)
        
        mean_order=[f"{tac}_mean" for tac in desired_order_tactics]
        print(f"MEAN ORDER:{mean_order}")
        df_scores_mean=df_scores_mean[mean_order]



    if get_factor_scores_by in ["weighted","both"]:
        fa_scores_weighted = fa.transform(data)#用和items fa相同的fa transforme data就可以得到加权weighted!的scores==tactics!
        df_scores_weighted=pd.DataFrame(fa_scores_weighted).reset_index(drop=True)
        # print(f"[INFO] scores weighted df:")
        # display(df_scores_weighted.head())
        
        if FA_NAMES:
            df_scores_weighted.columns=[f"{col}_pondéré" for col in FA_NAMES]
            #reorder:
            weighted_order=[f"{tac}_pondéré" for tac in desired_order_tactics]
            print(f"WEIGHTED ORDER:{weighted_order}")
            df_scores_weighted=df_scores_weighted[weighted_order]


    if get_factor_scores_by not in ['mean','weighted', 'both']:
        print(f"[WARNING] choose from 'mean' or 'weighted' or 'both'!!")

    df_scores=pd.concat([df_scores_mean, df_scores_weighted],axis=1)       



    print("------------------------heatmap of tactics (fa scores)----------------------------")
    
    # plot
    def heatmap_tactics(df_scores, run_scores_fa_on, OUTPUT_FOLDER):
        title_map={'mean':'moyens',
                   'weighted':'pondérés'}
        
        # init fa & fit 
        fa_scores = FactorAnalyzer(n_factors=2, rotation=ROTATION, method='minres') 
        fa_scores.fit(df_scores)
        
        # data
        loadings_scores = fa_scores.loadings_
        loadings_scores_df=pd.DataFrame(loadings_scores, index=df_scores.columns)
        display(loadings_scores_df)
        # save as latex
        outpath_scores_fa_latex=os.path.join(OUTPUT_FOLDER, "latex",f"tactics_{run_scores_fa_on}_fa.tex")        
        save_csv_as_latex(table_csv=loadings_scores_df, output_path=outpath_scores_fa_latex,
                          caption="Tableau de l'analyse factorielle des tactiques",
                          label="tab: tactics_fa",
                          round=3, escape=True, index=True)       
    
         
        # print(f"[SAVE] table of {title_map.get(run_scores_fa_on,None)} scores saved to {outpath_scores_fa_latex}!\n")

        # save as csv :?
        
        
        
        # plot
        sns.heatmap(loadings_scores_df, annot=True, cmap='coolwarm')
        
        plt.title(f'Charge factorielles des tactiques (scores {title_map.get(run_scores_fa_on,None)})')
        plt.tight_layout()
        #
        outpath_scores_heatmap=os.path.join(OUTPUT_FOLDER,f'heatmap_fa_scores_{run_scores_fa_on}.jpg')
        plt.savefig(outpath_scores_heatmap,dpi=300)
        plt.show()
        print(f"[SAVE] heatmap of {title_map.get(run_scores_fa_on,None)} scores saved to {outpath_scores_heatmap}!\n")
        return
    
    
    
    if run_scores_fa_on in ['mean', "both"] and not df_scores_mean.empty:   
        heatmap_tactics(df_scores=df_scores_mean, run_scores_fa_on='mean', OUTPUT_FOLDER=OUTPUT_FOLDER)
    
    if run_scores_fa_on in ['weighted', "both"] and not df_scores_weighted.empty:
        heatmap_tactics(df_scores=df_scores_weighted, run_scores_fa_on="weighted", OUTPUT_FOLDER=OUTPUT_FOLDER)
    
    if run_scores_fa_on not in ["mean",'weighted', "both"] :
        print(f"[INFO] choisir between 'mean','weighted','both' to run fa on tactics' scores!\n")


    print("----------------------------merge data to listings(keep nan)---------------------------------")
    df_dropna = df_dropna.reset_index(drop=True)
    df_scores=df_scores.reset_index(drop=True)
    df_tactics=pd.concat([df_dropna, df_scores], axis=1)# horizontal
    
    df_bio_tactics=df_tactics.drop_duplicates(subset="host_about")
    cols_tosave=["host_about"]+df_scores.columns.tolist()
    df_bio_tactics=df_bio_tactics[cols_tosave]
    listings_tactics=DF.merge(df_bio_tactics, left_on='host_about', right_on='host_about', how="left")
    
    outpath_listings_tactics=os.path.join(OUTPUT_FOLDER, 'lisitngs_tactics.csv')
    listings_tactics.to_csv(outpath_listings_tactics, index=False)
    display(listings_tactics.head())
    print(f"[INFO] scores shape (len(data), 1/2*N_FACTORS) :{df_scores.shape}\n"
            f"len(df_dropna) should == len(df_scores)==len(df_tactics): {len(df_dropna)}, {len(df_scores)}, {len(df_tactics)}\n"
            f"len(DF)==len(listings_tactics):{len(DF)},{len(listings_tactics)}\n")
    print(f"[SAVE] listings with tactics scores by '{get_factor_scores_by}'way saved to {outpath_listings_tactics}!!\n")
            
        
    
    print("###====================================CHECK=======================================###")   

    if get_cronbachs_a==True:
        print("---------------------------cronbach's alpha-------------------------------")
        print(f"[INFO] good alpha > 0.7\n")
        print(f"Cronbach's alpha checks internal consistency, whether items mesure a lantent construct !\n")
        
        # predifine mapping dict
        items2tac={
            "openness":['open to different cultures','cosmopolitan','international view','cultural exchange'],
            "authenticity":['personal life','life experiences','divers interests','hobbies','enjoy life'],
            'sociability':['meet new people', 'welcoming', 'friendly','sociable', 'interpersonal interaction'],
            'self_promotion':['thoughtful service', 'attentive to needs','willing to help','responsive'],
            'exemplification':["fan of Airbnb","Airbnb community",'love Airbnb', 'travel with Airbnb']
        }
        dict_alpha={}
        for tactic, items in items2tac.items():
            alpha, _ = cronbach_alpha(data[items])
            # print(f"- {tactic}: {alpha}")          
            dict_alpha[tactic]=alpha
        dict_alpha_df = pd.Series(dict_alpha, name='cronbach_alpha').reset_index()
        dict_alpha_df.columns = ['tactique', 'cronbach_alpha']
        display(dict_alpha_df)
        outpath_alpha=os.path.join(OUTPUT_FOLDER, "cronbachs_alpha.csv")
        dict_alpha_df.to_csv(outpath_alpha, index=False)
        print(f"[SAVE] cronbach's alpha on items saved to {outpath_alpha}!\n")


        
        
    if get_barplot==True:        
        print(f"\n-----------------------------barplot comm&uniq-------------------------------")
        communalities = fa.get_communalities()
        uniqueness = fa.get_uniquenesses()
        print(f"[INFO] mean COMM (> 0.6):{communalities.mean()}\n"
            f"[INFO] mean UNIQ : {uniqueness.mean()}\n")
        
        
        comm_df = pd.DataFrame({
            'Communality': communalities,
            'Uniqueness': uniqueness
        }, index=data.columns)
        comm_df = comm_df.sort_values("Communality")

        plt.figure(figsize=(10,6))
        colors = sns.color_palette("viridis", n_colors=2)  # 两个指标

        comm_df.plot(kind='bar', stacked=False, color=colors)# edgecolor='black'
        plt.title("Communalités et spécificités des items", fontsize=16)
        plt.ylabel("Proportion de la Variance Expliquée")
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Metric')
        plt.tight_layout()
        outpath_barplot=os.path.join(OUTPUT_FOLDER,"comm_uniq_barplot.jpg")
        plt.savefig(outpath_barplot, dpi=300)
        plt.show()
        print(f"[SAVE] barplot saved to {outpath_barplot}!\n")


    
    
    if get_pca3d==True:
        print("# -----------------------------3D PCA-------------------------------")
        pca_3d=PCA(n_components=3)
        loadings_3d=pca_3d.fit_transform(loadings)
        fig = plt.figure(figsize=(15,15))
        ax = fig.add_subplot(111, projection='3d')
        
        # 缩放向量
        norms = np.linalg.norm(loadings_3d, axis=1)
        loadings_3d_scaled = loadings_3d / norms[:, np.newaxis] #0.8
        print("[INFO] Explained variance ratios 3D (>0.6):", pca_3d.explained_variance_ratio_.sum())

        # 画箭头
        for i, var in enumerate(data.columns):
            ax.quiver(0, 0, 0,
                    loadings_3d_scaled[i, 0],
                    loadings_3d_scaled[i, 1],
                    loadings_3d_scaled[i, 2],
                    color='red', arrow_length_ratio=0.1)

            ax.text(loadings_3d_scaled[i, 0]*1.1,
                    loadings_3d_scaled[i, 1]*1.1,
                    loadings_3d_scaled[i, 2]*1.1,
                    var, fontsize=10)

        # 坐标轴限制
        ax.set_xlim(-1, 1)
        ax.set_ylim(-1, 1)
        ax.set_zlim(-1, 1)
        ax.set_xlabel('PC1')
        ax.set_ylabel('PC2')
        ax.set_zlabel('PC3')

        plt.title('3D PCA Correlation Circle (Scaled)')
        plt.tight_layout()
        outpath_pca=os.path.join(OUTPUT_FOLDER, 'pca_3d.jpg')
        plt.savefig(outpath_pca,dpi=300)
        plt.show()
        print(f"[SAVE] PCA 3D saved to {outpath_pca}!\n")
        
        



    
        
        
    return 